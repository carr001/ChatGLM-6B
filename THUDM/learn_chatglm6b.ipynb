{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b59ca-6c89-496c-aaab-fc114eff6743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d7e1a7-f1d4-4f49-b6c5-a5d447641f0a",
   "metadata": {},
   "source": [
    "# tokenization_chatglm6b.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e725bf-1c6d-4385-ae19-cf39810ab764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "import os\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils import logging, PaddingStrategy\n",
    "from transformers.tokenization_utils_base import EncodedInput, BatchEncoding\n",
    "from typing import Dict\n",
    "import sentencepiece as spm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934773bd-e452-40bf-821a-8037eb6f5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(model_path)\n",
    "        self.num_tokens = self.sp.vocab_size()\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.sp.EncodeAsIds(text)\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        return self.sp.DecodeIds(ids)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.sp.EncodeAsPieces(text)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return self.sp.DecodePieces(tokens)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.sp.PieceToId(token) for token in tokens]\n",
    "\n",
    "    def convert_token_to_id(self, token):\n",
    "        return self.sp.PieceToId(token)\n",
    "\n",
    "    def convert_id_to_token(self, idx):\n",
    "        return self.sp.IdToPiece(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bbeece7-1bc8-4a83-830b-83e94ce85b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPTokenizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            num_image_tokens=20000,\n",
    "            max_blank_length=80,\n",
    "            byte_fallback=True,\n",
    "    ):\n",
    "        assert vocab_file is not None\n",
    "        self.vocab_file = vocab_file\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        self.special_tokens = [\"[MASK]\", \"[gMASK]\", \"[sMASK]\", \"<unused_0>\", \"<sop>\", \"<eop>\", \"<ENC>\", \"<dBLOCK>\"]\n",
    "        self.max_blank_length = max_blank_length\n",
    "        self.byte_fallback = byte_fallback\n",
    "        self.text_tokenizer = TextTokenizer(vocab_file)\n",
    "\n",
    "    def _get_text_tokenizer(self):\n",
    "        return self.text_tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def get_blank_token(length: int):\n",
    "        assert length >= 2\n",
    "        return f\"<|blank_{length}|>\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tab_token():\n",
    "        return f\"<|tab|>\"\n",
    "\n",
    "    @property\n",
    "    def num_text_tokens(self):\n",
    "        return self.text_tokenizer.num_tokens\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return self.num_image_tokens + self.num_text_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_whitespaces(text: str, max_len: int = 80):\n",
    "        text = text.replace(\"\\t\", SPTokenizer.get_tab_token())\n",
    "        for i in range(max_len, 1, -1):\n",
    "            text = text.replace(\" \" * i, SPTokenizer.get_blank_token(i))\n",
    "        return text\n",
    "\n",
    "    def _preprocess(self, text: str, linebreak=True, whitespaces=True):\n",
    "        if linebreak:\n",
    "            text = text.replace(\"\\n\", \"<n>\")\n",
    "        if whitespaces:\n",
    "            text = self._encode_whitespaces(text, max_len=self.max_blank_length)\n",
    "        return text\n",
    "\n",
    "    def encode(\n",
    "            self, text: str, linebreak=True, whitespaces=True, add_dummy_prefix=True\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        @param text: Text to encode.\n",
    "        @param linebreak: Whether to encode newline (\\n) in text.\n",
    "        @param whitespaces: Whether to encode multiple whitespaces or tab in text, useful for source code encoding.\n",
    "        @param special_tokens: Whether to encode special token ([MASK], [gMASK], etc.) in text.\n",
    "        @param add_dummy_prefix: Whether to add dummy blank space in the beginning.\n",
    "        \"\"\"\n",
    "        text = self._preprocess(text, linebreak, whitespaces)\n",
    "        if not add_dummy_prefix:\n",
    "            text = \"<n>\" + text\n",
    "        tmp = self._get_text_tokenizer().encode(text)\n",
    "        tokens = [x + self.num_image_tokens for x in tmp]\n",
    "        return tokens if add_dummy_prefix else tokens[2:]\n",
    "\n",
    "    def postprocess(self, text):\n",
    "        text = text.replace(\"<n>\", \"\\n\")\n",
    "        text = text.replace(SPTokenizer.get_tab_token(), \"\\t\")\n",
    "        for i in range(2, self.max_blank_length + 1):\n",
    "            text = text.replace(self.get_blank_token(i), \" \" * i)\n",
    "        return text\n",
    "\n",
    "    def decode(self, text_ids: List[int]) -> str:\n",
    "        ids = [int(_id) - self.num_image_tokens for _id in text_ids]\n",
    "        ids = [_id for _id in ids if _id >= 0]\n",
    "        text = self._get_text_tokenizer().decode(ids)\n",
    "        text = self.postprocess(text)\n",
    "        return text\n",
    "\n",
    "    def decode_tokens(self, tokens: List[str]) -> str:\n",
    "        text = self._get_text_tokenizer().convert_tokens_to_string(tokens)\n",
    "        text = self.postprocess(text)\n",
    "        return text\n",
    "\n",
    "    def tokenize(\n",
    "            self, text: str, linebreak=True, whitespaces=True, add_dummy_prefix=True\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        @param text: Text to encode.\n",
    "        @param linebreak: Whether to encode newline (\\n) in text.\n",
    "        @param whitespaces: Whether to encode multiple whitespaces or tab in text, useful for source code encoding.\n",
    "        @param special_tokens: Whether to encode special token ([MASK], [gMASK], etc.) in text.\n",
    "        @param add_dummy_prefix: Whether to add dummy blank space in the beginning.\n",
    "        \"\"\"\n",
    "        text = self._preprocess(text, linebreak, whitespaces)\n",
    "        if not add_dummy_prefix:\n",
    "            text = \"<n>\" + text\n",
    "        tokens = self._get_text_tokenizer().tokenize(text)\n",
    "        return tokens if add_dummy_prefix else tokens[2:]\n",
    "\n",
    "    def __getitem__(self, x: Union[int, str]):\n",
    "        if isinstance(x, int):\n",
    "            if x < self.num_image_tokens:\n",
    "                return \"<image_{}>\".format(x)\n",
    "            else:\n",
    "                return self.text_tokenizer.convert_id_to_token(x - self.num_image_tokens)\n",
    "        elif isinstance(x, str):\n",
    "            if x.startswith(\"<image_\") and x.endswith(\">\") and x[7:-1].isdigit():\n",
    "                return int(x[7:-1])\n",
    "            else:\n",
    "                return self.text_tokenizer.convert_token_to_id(x) + self.num_image_tokens\n",
    "        else:\n",
    "            raise ValueError(\"The key should be str or int.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc6e356-3ef7-408b-be49-0b3ad2f01992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_whitespaces(text: str, max_len: int = 80):\n",
    "    text = text.replace(\"\\t\", SPTokenizer.get_tab_token())\n",
    "    for i in range(max_len, 1, -1):\n",
    "        text = text.replace(\" \" * i, SPTokenizer.get_blank_token(i))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0599bc3c-2a73-4139-84e9-f0c1b5ee8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An<|blank_5|><|blank_3|>apple\n",
      "An<|tab|><|blank_5|><|blank_3|>apple\n"
     ]
    }
   ],
   "source": [
    "print(_encode_whitespaces(\"An        apple\", 5))\n",
    "print(_encode_whitespaces(\"An\\t        apple\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65e898c-d652-4a2d-a015-8a5cf8c1c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|tab|>\n",
      "<|blank_3|>\n"
     ]
    }
   ],
   "source": [
    "print(SPTokenizer.get_tab_token())\n",
    "print(SPTokenizer.get_blank_token(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb35e7b-1aa1-4446-a2f4-a3f7dc0c3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd95588-af7c-4a21-9698-1a69bf77f040",
   "metadata": {},
   "source": [
    "## BPE algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f36d6356-36cc-4a67-9852-2f2abaad37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word, freq in collections.Counter(text).items():\n",
    "        vocab[' '.join(word) + ' </w>'] = freq\n",
    "    return vocab\n",
    "\n",
    "def bpe(text, num_merges):\n",
    "    vocab = get_vocab(text)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab\n",
    "\n",
    "# Test the BPE function\n",
    "text = ['low', 'low', 'low', 'low', 'low', 'lower', 'lower', 'newest', 'newest', 'newest', 'newest', 'newest', 'newest', 'widest', 'widest', 'widest']\n",
    "print(bpe(text, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69cb553f-6f13-423f-a0a9-96a9de28641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "print(bpe(text, 1))\n",
    "print(bpe(text, 2))\n",
    "print(bpe(text, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0461e0e-7b79-42c3-a77f-068d65097060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d76a4a6-d6e9-42a1-ac3e-c08903b756c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 11, ('r', '</w>'): 11, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3})\n",
      "('e', 'r')\n"
     ]
    }
   ],
   "source": [
    "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e r </w>': 6, 'w i d e r </w>': 3}\n",
    "pairs = get_stats(vocab)\n",
    "print(pairs)\n",
    "best = max(pairs, key=pairs.get)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f3bbd7d-9a12-4646-ac70-505c2264ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, in_vocab):\n",
    "    out_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in in_vocab:\n",
    "        out_word = p.sub(''.join(pair), word)\n",
    "        out_vocab[out_word] = in_vocab[word]\n",
    "    return out_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "608b0206-c024-4d8e-ba35-73910af353dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l o w </w>': 5, 'l o w er </w>': 2, 'n e w er </w>': 6, 'w i d er </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "vocab = merge_vocab(best, vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d43f07-2079-4d15-b795-641bbace28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\\ r\n"
     ]
    }
   ],
   "source": [
    "bigram = re.escape(' '.join(best))\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca19ce-c91c-4497-9318-4d06163f2ce6",
   "metadata": {},
   "source": [
    "# HF tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa82ae6-3665-4c1e-90ba-7285607d970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '你好', ',', '我是', '惠', '成', '煊', ',', '请问', '你是谁', '?']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"F:\\\\learn\\\\AI\\\\carr001\\\\learn_ai\\\\third_party\\\\ChatGLM-6b\\\\THUDM\\\\chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer.tokenize(\"你好，我是惠成煊，请问你是谁？\")\n",
    "\n",
    "# tokenize更多的方法可以在SentencePiece中的python实现中看到\n",
    "# 参考https://github.com/google/sentencepiece/blob/master/python/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fe11b72-738d-4e69-b997-bebc62878215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<s>': 1}\n",
      "{'ŋ': 129944, 'ක': 129945, '작': 129946, '\\x98': 129947, 'ය': 129948}\n",
      "2\n",
      "('你好，我是惠成煊，请问你是  谁？', {})\n",
      "('hello, my name is carr, how are you ?', {})\n",
      "130001\n",
      "<bound method SpecialTokensMixin.add_tokens of ChatGLMTokenizer(name_or_path='F:\\learn\\AI\\carr001\\learn_ai\\third_party\\ChatGLM-6b\\THUDM\\chatglm-6b', vocab_size=130344, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130004: AddedToken(\"<sop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130005: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(dict(list(vocab.items())[:2]))\n",
    "print(dict(list(vocab.items())[-400:-395]))\n",
    "print(tokenizer.num_special_tokens_to_add())\n",
    "print(tokenizer.prepare_for_tokenization(\"你好，我是惠成煊，请问你是  谁？\"))\n",
    "print(tokenizer.prepare_for_tokenization(\"hello, my name is carr, how are you ?\"))\n",
    "print(tokenizer.gmask_token_id)\n",
    "print(tokenizer.add_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c236774-64d1-4553-af6f-eac9017266fb",
   "metadata": {},
   "source": [
    "- bos_token（Begin Of Sentence token）：句子开头的标记，用来指示一个句子的开始。\n",
    "- eos_token（End Of Sentence token）：句子结束的标记，用来指示一个句子的结束。\n",
    "- end_token：这通常与eos_token相同，用来指示一个序列的结束，比如句子的结尾。\n",
    "- gmask_token：这是一个特定于某些模型（如T5模型）的令牌，用来指示一个序列中的部分内容需要被模型生成或预测。\n",
    "- mask_token：在屏蔽语言模型（如BERT）中使用的令牌，用来替换文本中的某些词，以训练模型对被屏蔽词的上下文进行理解。\n",
    "- pad_token：填充令牌，用来将不同长度的文本序列填充到相同的长度，以便可以批量处理。在许多模型中，序列需要被填充到批处理中最长序列的长度。\n",
    "- unk_token（Unknown token）：未知令牌，用来替换模型词汇表之外的词。当模型遇到训练期间未见过的词时，会用unk_token来表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26143be-6e5b-4106-8804-326966ed1e3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278463a1-a8c7-414a-825e-86e9d1b84010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7f9445-1c2a-4046-8cef-c1ca7736787d",
   "metadata": {},
   "source": [
    "# chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ba56219-c7c3-471c-b402-4223041e8eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d599e29f0b3941aeb3789eb7b75c6f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"F:\\\\learn\\\\AI\\\\carr001\\\\learn_ai\\\\third_party\\\\ChatGLM-6b\\\\THUDM\\\\chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"F:\\\\learn\\\\AI\\\\carr001\\\\learn_ai\\\\third_party\\\\ChatGLM-6b\\\\THUDM\\\\chatglm-6b\", trust_remote_code=True).half().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88a6d066-508c-471e-91c1-14736739cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e59517f-8f68-4f3e-ac00-a81965bc99df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'作为一个人工智能助手，我没有个人喜好或情感，因为我只是由计算机程序驱动的。我的目的是尽可能准确地回答你的问题和提供帮助，所以请随时问我问题。'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你喜欢做什么\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa87418b-5a45-4c09-a9d6-92209e5ecd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I\\'d be happy to say a sentence in English. How about \"Hello, world!\" as a starting point?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你说句英文\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e576191-6f51-48c5-b1ae-74641d7dcf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
